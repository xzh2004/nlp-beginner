## 任务五：基于神经网络的语言模型

实现了基于LSTM和GRU的字符级语言模型，用于生成唐诗风格的文本。

### 项目结构

```
lab5/
├── data_loader.py      # 数据加载和预处理
├── model.py           # LSTM和GRU模型定义
├── trainer.py         # 训练器
├── text_generator.py  # 文本生成器
├── train.py          # 主训练脚本
├── evaluate.py       # 模型评估脚本
├── generate.py       # 文本生成脚本
└── README.md        # 项目说明
```

### 模型架构

#### LSTM语言模型

- **字符嵌入层**：字符级向量表示
- **多层LSTM**：捕获长期依赖
- **Dropout层**：防止过拟合
- **输出层**：字符概率分布

#### GRU语言模型

- **字符嵌入层**：字符级向量表示
- **多层GRU**：简化版循环网络
- **Dropout层**：防止过拟合
- **输出层**：字符概率分布

### 训练策略

- **损失函数**：交叉熵损失
- **优化器**：Adam优化器
- **学习率调度**：动态调整学习率
- **梯度裁剪**：防止梯度爆炸

### 文本生成

支持多种生成方式：

- **自由生成**：随机生成唐诗风格文本
- **提示生成**：根据给定提示生成文本（效果欠佳）
- **诗歌生成**：生成特定格式的诗歌（默认七言绝句）

### 实验结果

测试集困惑度: 484.27
测试集损失: 6.1826

### 生成示例

```
圃不，
林人有之上。
高知何有山，
水卧跨沙载。
重知为未堂，
何君青活里。
```

### 实验讨论

实验结果表明，当前模型的诗歌生成效果未能达到预期水平。经过深入分析，我认为主要存在以下三个关键问题：

**1. 数据集规模限制**
训练数据集规模相对较小，无法为深度学习模型提供充足的训练样本，导致模型难以充分学习诗歌的语言模式和韵律特征。

**2. 模型架构复杂度不足**
当前采用的LSTM模型架构相对简单，参数量有限，缺乏足够的表达能力来捕捉诗歌创作中的复杂语义关系和韵律规律。

**3. 数据格式多样性导致的训练困难**
训练数据集中包含多种诗歌体裁（如绝句、律诗等），不同体裁的句子长度和标点符号使用规则存在显著差异。这种格式多样性使得神经网络难以准确掌握标点符号的分布规律，进而影响生成诗歌的格式规范性。

**改进建议**
针对上述问题，我建议采用**分体裁训练**的策略，即针对不同诗歌体裁（如绝句、律诗）分别训练专门的模型。这种方法能够确保每个模型专注于学习特定体裁的格式特征，从而提高生成诗歌的格式准确性和整体质量。

### 参考资料：

 https://github.com/YC-Coder-Chen/Tang-Poetry-Generator/tree/master